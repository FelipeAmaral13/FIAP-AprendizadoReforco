{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Equação de Bellman\n",
    "\n",
    "A Equação de Bellman é um conceito fundamental no campo do Aprendizado por Reforço, que é uma abordagem de aprendizado de máquina em que um agente aprende a tomar decisões interativas para maximizar suas recompensas ao interagir com um ambiente. A equação leva o nome do matemático Richard Bellman, que contribuiu significativamente para a teoria dos processos de decisão estocásticos.\n",
    "\n",
    "A Equação de Bellman no contexto do Aprendizado por Reforço é uma fórmula que expressa como o valor esperado de um estado está relacionado aos valores esperados dos estados futuros, levando em consideração as recompensas imediatas e as possíveis transições de estado. Ela descreve como um agente deve avaliar o valor de um estado com base nas recompensas futuras esperadas, levando em conta as ações que o agente pode escolher.\n",
    "\n",
    "A formulação básica da Equação de Bellman é a seguinte:\n",
    "\n",
    "`V(s) = max_a [ R(s, a) + γ * Σ_s' [ P(s' | s, a) * V(s') ] ]`\n",
    "\n",
    "Onde:\n",
    "\n",
    "* V(s) é o valor esperado do estado s.\n",
    "* max_a denota a maximização sobre todas as ações possíveis a no estado s.\n",
    "* R(s, a) é a recompensa imediata obtida ao executar a ação a no estado s.\n",
    "* γ é o fator de desconto que pondera as recompensas futuras em relação às recompensas imediatas.\n",
    "* P(s' | s, a) é a probabilidade de transição para o estado s' a partir do estado s ao executar a ação a.\n",
    "* V(s') é o valor esperado do estado futuro s'.\n",
    "\n",
    "Em resumo, a Equação de Bellman é uma ferramenta crucial para avaliar o valor de diferentes estados e guiar as decisões do agente para maximizar suas recompensas ao longo do tempo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importar bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definir Parâmetros do Jogo\n",
    "\n",
    "Aqui, definimos os parâmetros básicos do jogo, incluindo o tamanho do grid, o estado inicial do agente, o estado do objetivo (moeda) e a taxa de desconto (gamma) usada na equação de Bellman."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = 4\n",
    "cols = 4\n",
    "start_state = (0, 0)\n",
    "goal_state = (3, 3)\n",
    "gamma = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inicializar Valores de Estado (V)\n",
    "\n",
    "Inicializamos uma matriz V de tamanho (rows, cols) com zeros. Essa matriz representa os valores de estado do agente, ou seja, a estimativa de recompensa que o agente espera receber a partir de cada estado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = np.zeros((rows, cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definir Função de Recompensa\n",
    "\n",
    "Aqui, definimos uma função de recompensa que retorna 10 se o estado for o estado do objetivo (moeda) e -1 caso contrário."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward(state):\n",
    "    if state == goal_state:\n",
    "        return 10\n",
    "    else:\n",
    "        return -1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definir Movimentos Possíveis\n",
    "\n",
    "Criamos uma lista de ações possíveis que o agente pode tomar: mover-se para cima, para baixo, para a esquerda ou para a direita."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = [(0, -1), (0, 1), (-1, 0), (1, 0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algoritmo de Aprendizado por Reforço\n",
    "\n",
    "Neste passo, implementamos o algoritmo de aprendizado por reforço usando a equação de Bellman. Realizamos iterações para atualizar os valores de estado (V) com base nas recompensas esperadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 1000  # Número de iterações do algoritmo de aprendizado por reforço\n",
    "\n",
    "# Loop principal que executa o algoritmo de aprendizado por reforço várias vezes\n",
    "for _ in range(num_iterations):\n",
    "    new_V = np.copy(V)  # Cria uma cópia dos valores de estado atuais para atualização\n",
    "\n",
    "    # Loop para percorrer cada posição no grid\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            state = (i, j)  # Define o estado atual (posição atual no grid)\n",
    "\n",
    "            # Verifica se o estado atual é o estado do objetivo\n",
    "            if state == goal_state:\n",
    "                continue  # Se for, pula para a próxima iteração (não faz nada neste estado)\n",
    "\n",
    "            max_value = float(\"-inf\")  # Inicializa o valor máximo com um valor negativo infinito\n",
    "\n",
    "            # Loop para percorrer cada ação possível (movimento) no estado atual\n",
    "            for action in actions:\n",
    "                new_i = i + action[0]  # Calcula a nova linha após a ação\n",
    "                new_j = j + action[1]  # Calcula a nova coluna após a ação\n",
    "\n",
    "                # Verifica se a nova posição está dentro dos limites do grid\n",
    "                if 0 <= new_i < rows and 0 <= new_j < cols:\n",
    "                    new_state = (new_i, new_j)  # Define o novo estado após a ação\n",
    "                    value = reward(state) + gamma * V[new_state[0], new_state[1]]\n",
    "                    # Calcula o valor atualizado do estado usando a equação de Bellman\n",
    "                    # (recompensa imediata + fator de desconto * valor do novo estado)\n",
    "                    max_value = max(max_value, value)  # Mantém o valor máximo encontrado\n",
    "\n",
    "            new_V[i, j] = max_value  # Atualiza o valor do estado atual na nova matriz de valores\n",
    "\n",
    "    V = new_V  # Atualiza a matriz de valores de estado com os novos valores calculados\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definir a Política do Agente\n",
    "\n",
    "Aqui, definimos a política do agente, ou seja, a ação que o agente deve escolher em cada estado para maximizar sua recompensa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicialização da matriz de política com valores vazios (\"\")\n",
    "policy = np.zeros((rows, cols), dtype=str)\n",
    "\n",
    "# Loop para iterar pelas linhas do grid\n",
    "for i in range(rows):\n",
    "    # Loop para iterar pelas colunas do grid\n",
    "    for j in range(cols):\n",
    "        # Verifica se a posição atual é o estado objetivo\n",
    "        if (i, j) == goal_state:\n",
    "            # Se for o estado objetivo, atribui \"G\" à política nessa posição\n",
    "            policy[i, j] = \"G\"\n",
    "        else:\n",
    "            # Inicializa o valor máximo como negativo infinito\n",
    "            max_value = float(\"-inf\")\n",
    "            # Inicializa a ação ótima como vazia (None)\n",
    "            best_action = None\n",
    "            # Loop para iterar pelas possíveis ações\n",
    "            for action in actions:\n",
    "                # Calcula a nova posição após a ação\n",
    "                new_i = i + action[0]\n",
    "                new_j = j + action[1]\n",
    "                # Verifica se a nova posição é válida dentro dos limites do grid\n",
    "                if 0 <= new_i < rows and 0 <= new_j < cols:\n",
    "                    # Calcula o novo estado após a ação\n",
    "                    new_state = (new_i, new_j)\n",
    "                    # Calcula o valor usando a Equação de Bellman para a nova posição\n",
    "                    value = reward((i, j)) + gamma * V[new_state[0], new_state[1]]\n",
    "                    # Atualiza o valor máximo e a ação ótima se o valor for maior\n",
    "                    if value > max_value:\n",
    "                        max_value = value\n",
    "                        best_action = action\n",
    "            # Atribui um símbolo à política com base na ação ótima encontrada\n",
    "            if best_action == (0, -1):\n",
    "                policy[i, j] = \"↑\"  # Ação de mover para cima\n",
    "            elif best_action == (0, 1):\n",
    "                policy[i, j] = \"↓\"  # Ação de mover para baixo\n",
    "            elif best_action == (-1, 0):\n",
    "                policy[i, j] = \"←\"  # Ação de mover para esquerda\n",
    "            elif best_action == (1, 0):\n",
    "                policy[i, j] = \"→\"  # Ação de mover para direita\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Imprimir Valores Aprendidos para Cada Estado\n",
    "\n",
    "Este trecho de código imprime os valores de estado aprendidos para cada posição no grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V(0,0): -4.69\tV(0,1): -4.10\tV(0,2): -3.44\tV(0,3): -2.71\t\n",
      "V(1,0): -4.10\tV(1,1): -3.44\tV(1,2): -2.71\tV(1,3): -1.90\t\n",
      "V(2,0): -3.44\tV(2,1): -2.71\tV(2,2): -1.90\tV(2,3): -1.00\t\n",
      "V(3,0): -2.71\tV(3,1): -1.90\tV(3,2): -1.00\tV(3,3): 0.00\t\n"
     ]
    }
   ],
   "source": [
    "for i in range(rows):\n",
    "    for j in range(cols):\n",
    "        print(f\"V({i},{j}): {V[i,j]:.2f}\", end=\"\\t\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imprimir a Política do Agente\n",
    "Este trecho de código imprime a política final do agente, mostrando a ação escolhida em cada posição do grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "↓\t↓\t↓\t→\t\n",
      "↓\t↓\t↓\t→\t\n",
      "↓\t↓\t↓\t→\t\n",
      "↓\t↓\t↓\tG\t\n"
     ]
    }
   ],
   "source": [
    "for i in range(rows):\n",
    "    for j in range(cols):\n",
    "        print(policy[i, j], end=\"\\t\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_AR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
