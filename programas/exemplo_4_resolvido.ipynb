{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exemplo 3  - Taxi\n",
    "\n",
    "Existem quatro locais designados na grade indicados por R(ed), G(reen), Y(ellow) e B(lue).  Quando o episódio começa, o táxi parte em um quadrado aleatório e o passageiro está em um local aleatório.  O táxi dirige até o local do passageiro, pega o passageiro, dirige até o destino do passageiro (outro dos quatro locais especificados) e então deixa o passageiro.  Assim que o passageiro é deixado, o episódio termina.\n",
    "\n",
    "\n",
    "## Ações\n",
    "\n",
    " Existem 6 ações determinísticas discretas:\n",
    "\n",
    "     0: mover para o sul\n",
    "\n",
    "     1: mover para o norte\n",
    "\n",
    "     2: mover para o leste\n",
    "\n",
    "     3: mover para o oeste\n",
    "\n",
    "     4: passageiro de coleta\n",
    "\n",
    "     5: deixar o passageiro\n",
    "\n",
    "\n",
    "## Observações\n",
    "\n",
    " Existem 500 estados discretos, pois existem 25 posições de táxi, 5 localizações possíveis do passageiro (incluindo o caso em que o passageiro está no táxi) e 4 localizações de destino.\n",
    "\n",
    " Observe que existem 400 estados que podem ser alcançados durante um episódio.  Os estados ausentes correspondem a situações em que o passageiro está no mesmo local de seu destino, pois isso normalmente sinaliza o fim de um episódio.  Quatro estados adicionais podem ser observados logo após episódios de sucesso, quando tanto o passageiro quanto o táxi estão no destino.  Isso dá um total de 404 estados discretos alcançáveis.\n",
    "\n",
    " Cada espaço de estado é representado pela tupla: (taxi_row, taxi_col, passage_location, destination)\n",
    "\n",
    " Uma observação é um número inteiro que codifica o estado correspondente.  A tupla de estado pode então ser decodificada com o método “decode”.\n",
    "\n",
    " Localização dos passageiros:\n",
    "\n",
    "     0: R(ed)\n",
    "\n",
    "     1: G(real)\n",
    "\n",
    "     2: Y (amarelo)\n",
    "\n",
    "     3: B(lue)\n",
    "\n",
    "     4: no táxi\n",
    "\n",
    " Destinos:\n",
    "\n",
    "     0: R(ed)\n",
    "\n",
    "     1: G(real)\n",
    "\n",
    "     2: Y (amarelo)\n",
    "\n",
    "     3: B(lue)\n",
    "\n",
    "## Recompensas\n",
    "\n",
    " -1 por passo, a menos que outra recompensa seja acionada.\n",
    "\n",
    " +20 entregando passageiros.\n",
    "\n",
    " -10 executar ações de “pegar” e “devolver” ilegalmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando as bibliotecas necessárias\n",
    "import gym\n",
    "import time\n",
    "\n",
    "# Criando o ambiente FrozenLake-v1 com renderização visual\n",
    "env = gym.make(\"Taxi-v3\", render_mode='human')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# observation space - states \n",
    "print(env.observation_space)\n",
    " \n",
    "# actions: left -0, down - 1, right - 2, up- 3\n",
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reiniciando o ambiente e exibindo o estado inicial\n",
    "env.reset()\n",
    "env.render()\n",
    "\n",
    "print('Estado inicial do sistema')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo o número de iterações\n",
    "numberOfIterations = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomAction = env.action_space.sample()\n",
    "env.step(randomAction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executando interações com o ambiente\n",
    "for i in range(numberOfIterations):\n",
    "    # Gerando uma ação aleatória do espaço de ação\n",
    "    randomAction = env.action_space.sample()\n",
    "\n",
    "    # Executando a ação no ambiente e obtendo informações\n",
    "    observation, reward, terminated, truncated, info = env.step(randomAction)\n",
    "\n",
    "    # Exibindo o ambiente renderizado\n",
    "    env.render()\n",
    "\n",
    "    # Exibindo informações sobre a iteração\n",
    "    print(f'Iteração: {i+1}, ação {randomAction} resultou em: {observation, reward, terminated, truncated, info} ')\n",
    "\n",
    "    # Aguardando por um curto período de tempo\n",
    "    time.sleep(2)\n",
    "\n",
    "    # Verificando se o episódio foi concluído\n",
    "    if terminated:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fechando o ambiente\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_AR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
