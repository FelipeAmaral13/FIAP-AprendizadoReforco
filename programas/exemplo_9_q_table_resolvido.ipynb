{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aprendizado por Reforço com Q-Learning\n",
    "\n",
    "Neste Jupyter Notebook, implementaremos um exemplo simples de aprendizado por reforço usando o algoritmo Q-Learning. Vamos treinar um agente para tomar decisões em um ambiente com 4 estados e 2 ações possíveis. O agente aprenderá a maximizar as recompensas ao longo do tempo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importação de Bibliotecas e Inicialização\n",
    "\n",
    "Importação das bibliotecas e inicialização dos parâmetros iniciais do nosso ambiente de aprendizado por reforço, como o número de estados (4) e o número de ações (2). \n",
    "\n",
    "Em seguida, Criação de uma Q-table (tabela Q) com valores aleatórios para iniciar o processo de aprendizado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Defina o número de estados e ações possíveis no ambiente\n",
    "num_states = 4\n",
    "num_actions = 2\n",
    "\n",
    "# Inicialize a Q-table com valores aleatórios\n",
    "q_table = np.random.rand(num_states, num_actions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definição de Hiperparâmetros\n",
    "\n",
    "Definição dos hiperparâmetros que afetam o treinamento do agente. \n",
    "\n",
    "* learning_rate controla a taxa de aprendizado \n",
    "* discount_factor determina a importância das recompensas futuras em relação às recompensas imediatas\n",
    "* num_episodes especifica quantos episódios de treinamento realizaremos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defina hiperparâmetros para o algoritmo de aprendizado por reforço\n",
    "learning_rate = 0.1\n",
    "discount_factor = 0.9\n",
    "num_episodes = 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinamento do Agente\n",
    "\n",
    "É feito uma iteração por num_episodes, onde cada episódio começa com um estado inicial aleatório. \n",
    "\n",
    "O agente interage com o ambiente e escolhe ações com base em uma política epsilon-greedy (exploração vs. exploração). As recompensas são simuladas e usadas para atualizar a Q-table usando a equação de Bellman."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simule o ambiente com episódios de treinamento\n",
    "for episode in range(num_episodes):\n",
    "    # Inicialize o ambiente (estado inicial)\n",
    "    state = np.random.randint(0, num_states)\n",
    "    \n",
    "    # Execute o episódio até terminar\n",
    "    while True:\n",
    "        # Escolha uma ação com base na política epsilon-greedy\n",
    "        epsilon = 0.2  # Probabilidade de explorar ao invés de escolher a melhor ação\n",
    "        if np.random.uniform(0, 1) < epsilon:\n",
    "            action = np.random.randint(0, num_actions)  # Ação aleatória\n",
    "        else:\n",
    "            action = np.argmax(q_table[state, :])  # Escolha a ação com maior Q-value\n",
    "            \n",
    "        # Simule a ação no ambiente e receba a recompensa\n",
    "        next_state = (state + 1) % num_states  # Transição simples de estado para ilustração\n",
    "        reward = -1 if next_state != 3 else 10  # Recompensa final\n",
    "        \n",
    "        # Atualize a Q-table usando a equação de Bellman\n",
    "        q_table[state, action] = (1 - learning_rate) * q_table[state, action] + \\\n",
    "                                  learning_rate * (reward + discount_factor * np.max(q_table[next_state, :]))\n",
    "        \n",
    "        # Atualize o estado\n",
    "        state = next_state\n",
    "        \n",
    "        # Verifique se o episódio terminou\n",
    "        if state == 3:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exibição da Q-Table Final\n",
    "\n",
    "Cada valor na tabela representa o valor Q associado a uma ação em um estado específico. Isso nos permite interpretar como o agente aprendeu a valorizar diferentes ações em diferentes estados com base nas recompensas e nas políticas de exploração durante o treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-Table com Interpretação:\n",
      "Estado 0:\n",
      "  Ação 0: Q-value = 0.3916430528911931\n",
      "  Ação 1: Q-value = 0.11850142819284026\n",
      "\n",
      "Estado 1:\n",
      "  Ação 0: Q-value = 2.1501683653038253\n",
      "  Ação 1: Q-value = 0.8042213101199582\n",
      "\n",
      "Estado 2:\n",
      "  Ação 0: Q-value = 0.5051103180790946\n",
      "  Ação 1: Q-value = 7.126649041587006\n",
      "\n",
      "Estado 3:\n",
      "  Ação 0: Q-value = 0.6735571805539834\n",
      "  Ação 1: Q-value = 0.07296742463946193\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Imprima a Q-table final com interpretação\n",
    "print(\"Q-Table com Interpretação:\")\n",
    "for state in range(num_states):\n",
    "    print(f\"Estado {state}:\")\n",
    "    for action in range(num_actions):\n",
    "        print(f\"  Ação {action}: Q-value = {q_table[state, action]}\")\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_AR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
