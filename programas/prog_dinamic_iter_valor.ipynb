{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Definindo o ambiente Grid World\n",
    "# S: Estado inicial\n",
    "# G: Estado objetivo\n",
    "# #: Obstáculo\n",
    "# -: Caminho livre\n",
    "# A: Ação\n",
    "grid_world = [\n",
    "    ['#', '#', '#', '#', '#', '#', '#', '#', '#', '#'],\n",
    "    ['#', 'S', '#', '#', '#', '#', '#', '#', '#', '#'],\n",
    "    ['#', '-', '-', '-', '-', '-', '-', '#', '#', '#'],\n",
    "    ['#', '#', '#', '#', '#', '#', '-', '#', '#', '#'],\n",
    "    ['#', '#', '#', '#', '#', '#', 'A', '#', '#', '#'],\n",
    "    ['#', '#', '#', '#', '#', '#', '#', '#', 'G', '#'],\n",
    "    ['#', '#', '#', '#', '#', '#', '#', '#', '#', '#']\n",
    "]\n",
    "\n",
    "# Definindo parâmetros\n",
    "num_episodes = 500\n",
    "gamma = 0.9  # Fator de desconto\n",
    "alpha = 0.1  # Taxa de aprendizado\n",
    "\n",
    "# Convertendo o ambiente para uma matriz NumPy para facilitar a manipulação\n",
    "grid_world = np.array(grid_world)\n",
    "\n",
    "# Obtendo as coordenadas dos estados S, G e A\n",
    "start_state = np.argwhere(grid_world == 'S')[0]\n",
    "goal_state = np.argwhere(grid_world == 'G')[0]\n",
    "action_state = np.argwhere(grid_world == 'A')[0]\n",
    "\n",
    "# Função para escolher uma ação (movimento) aleatória\n",
    "def choose_action():\n",
    "    return np.random.choice(['up', 'down', 'left', 'right'])\n",
    "\n",
    "# Função para executar um episódio\n",
    "def run_episode():\n",
    "    state = start_state\n",
    "    episode = []\n",
    "\n",
    "    while tuple(state) != tuple(goal_state):\n",
    "        action = choose_action()\n",
    "        next_state = take_action(state, action)\n",
    "        reward = get_reward(next_state)\n",
    "        episode.append((state, action, reward))\n",
    "        state = next_state\n",
    "\n",
    "    return episode\n",
    "\n",
    "# Função para executar uma ação\n",
    "def take_action(state, action):\n",
    "    if action == 'up' and state[0] > 0 and grid_world[state[0] - 1, state[1]] != '#':\n",
    "        return state[0] - 1, state[1]\n",
    "    elif action == 'down' and state[0] < grid_world.shape[0] - 1 and grid_world[state[0] + 1, state[1]] != '#':\n",
    "        return state[0] + 1, state[1]\n",
    "    elif action == 'left' and state[1] > 0 and grid_world[state[0], state[1] - 1] != '#':\n",
    "        return state[0], state[1] - 1\n",
    "    elif action == 'right' and state[1] < grid_world.shape[1] - 1 and grid_world[state[0], state[1] + 1] != '#':\n",
    "        return state[0], state[1] + 1\n",
    "    else:\n",
    "        return state\n",
    "\n",
    "# Função para obter a recompensa\n",
    "def get_reward(state):\n",
    "    if tuple(state) == tuple(goal_state):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Função principal de Monte Carlo\n",
    "def monte_carlo():\n",
    "    state_values = np.zeros_like(grid_world, dtype=float)\n",
    "    state_counts = np.zeros_like(grid_world, dtype=int)\n",
    "\n",
    "    for episode_num in range(num_episodes):\n",
    "        episode = run_episode()\n",
    "        G = 0  # Retorno acumulado\n",
    "\n",
    "        for t in range(len(episode) - 1, -1, -1):\n",
    "            state, action, reward = episode[t]\n",
    "            G = gamma * G + reward\n",
    "\n",
    "            if state not in [x[0] for x in episode[:t]]:\n",
    "                state_counts[state] += 1\n",
    "                state_values[state] += (1 / state_counts[state]) * (G - state_values[state])\n",
    "\n",
    "    return state_values\n",
    "\n",
    "# Executando o Monte Carlo e obtendo os valores dos estados\n",
    "values = monte_carlo()\n",
    "\n",
    "# Visualizando os resultados\n",
    "plt.imshow(values, cmap='viridis', origin='upper')\n",
    "plt.colorbar()\n",
    "plt.title('Valores de Estado (Monte Carlo)')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_AR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
